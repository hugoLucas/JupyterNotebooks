{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Vanishing Gradients Problem** is a side effect of applying the Gradient Descent algorithm to deep neural networks wherein the back propogation error of the lower layers is so low that the algorithm leaves the weights of those layers virtually unchanged throughout the training phase. \n",
    "\n",
    "The **Exploding Gradients Problem** is a side effect of applying the Gradient Descent algorithm to deep neural networks wherein the back propogation error of the lower layers is so high that the algorithm updates the weights of those layers so substantially every iteration that the alogirthm diverges.  \n",
    "\n",
    "This problem can occur due to the difference in the variances of the sigmoid activation function and the normal distribution ($\\mu = 0$, $\\sigma = 1$) used to initialize layer weights. The use of these two functions causes the inputs of a layer to have lower variance than the output of the same layer. This difference eventually propogates enough that output variance is considerably higher in deeper layers when compared to the variance of the begining layers. This causes the inputs to have exteremly positive or negative values which in turn results in a derivative approcahing 0. Such a low derivative means the weights of a layer will remain unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Xavier and He Initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a signal to flow properly through a DNN, the variance of the inputs and outputs of each layer must remain virtually the same going in both directions (forward and back propogation). However this is in practice when the layer does not have an equal amount of input and output connections. Instead the connection weights should be intialized following the *Xavier Initlization* strategy when using a logistic activation function:\n",
    "\n",
    "\n",
    "$\\large \\text{Normal Distribution w/ } \\mu = 0 \\text{ and } \\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ \n",
    "\n",
    "OR\n",
    "\n",
    "$\\large \\text{Uniform Distribution between } [ -r, +r ] \\text{ where } r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$ \n",
    "\n",
    "Using this strategy speeds up the training of DNNs. For the ReLU activation function you can use *He activation*. By default the $fully\\_connected()$ function uses Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Nonsaturating Activation Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploding/vanishing gradients problems are in part due to a poor choice in activation function. The ReLU function behaves better in DNNs (as it does not saturate for positive values) when compared to the logistic function but it does have some drawbacks. An example of this is the **dying ReLU** problem wherein some nodes in your DNN simply die and output 0 no matter the input. \n",
    "\n",
    "The ReLU function has some variants. The *Leaky ReLU* function ($\\text{LeakyReLU}_{\\alpha}(z) = \\text{max}(\\alpha z, \\, z)$) uses the hyperparameter $\\alpha$ to avoid the problem of dying nodes. However it can result in nodes being in a coma, which essentially means they can die but come back to life if given enough time. \n",
    "\n",
    "Another activation function is the *exponential linear unit* (ELU): \n",
    "\n",
    "$\\large \\begin{gather*} \\text{ELU}_\\alpha (z) =\n",
    "\\begin{cases}\n",
    "  \\alpha \\, (e^{\\,z} - 1) & \\text{ if } z < 0\\\\    \n",
    "  z & \\text{ if } z \\geq 0 \\\\     \n",
    "\\end{cases}\n",
    "\\end{gather*}$\n",
    "\n",
    "The ELU function in general has an average output close to 0 (so no vanishing gradients problem), has a nonzero gradient when $z<0$ so nodes won't die, and is smooth everywhere so it speeds up convergence. However it is slower to compute than the ReLU function but this can be mitigated as it converges faster during training.  Both the leaky ReLU and the ELU functions can be used in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "def leaky_rlu(z, name=None):\n",
    "    return tf.maximum(0.01*z, z, name=name) \n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu) # ELU\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_rlu) # ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batch Normalization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Batch Normalization** algorithm consists of adding an operation in the model just before the activation of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer. This reduces the chances of the exploding/vanishing gradients problem occuring late into training. The algorithm is given below:\n",
    "\n",
    "1. $\\large \\quad \\mu_B = \\frac{1}{m_b} \\, \\Sigma_{i=1}^{m_B} \\, x^{(i)}$\n",
    "2. $\\large \\quad \\sigma_B^2 = \\frac{1}{m_b} \\, \\Sigma_{i=1}^{m_B} \\, (x^{(i)} - \\mu_B)^2$\n",
    "3. $\\large \\quad \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "4. $\\large \\quad z^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta$\n",
    "\n",
    "Using Batch Normalization will make your DNN more acurate, make it converge in fewer training iterations, and eliminate the need for regularization. However it make predictions slower, as there are more computations, and it makes your model more complex. \n",
    "\n",
    "Here's an implementation of this algorithm in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few lines are very repatative, this can be fixed using Python's partial function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the $training$ variable, when your model is making predictions you should remember to set this parameter to false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradient Clipping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gradient Clipping** technique is a technique wherein the backpropgation gradient is limited to some max and is *clipped* if the gradient is higher than the max. \n",
    "\n",
    "Do do this in tensorflow, you must clip the gradient value before calling $minimize()$ as that function will compute and apply an gradient. Therefore you must ask tensorflow to compute the gradient, clip it, and then apply it to the layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss) # Compute\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) # Clip\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs) # Apply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always try to borrow layers from another DNN that tackles a problem similar to yours. This will make training faster as you can fix the weights of those layers and only train the new portion of the DNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reusing a TensorFlow Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of reusing only specific portions of a previously trained TensorFlow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reusing Models from Other Frameworks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model uses another framework then you will have to load the weights into Python and assign to a layer in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [...] # Load the weights from the other framework\n",
    "original_b = [...] # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Freezing the Lower Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ignore a layer, simply tell the $minimize()$ function which variables to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tweaking, Dropping, or Replacing the Upper Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You almost always have to drop the output layer\n",
    "* Freeze all copied layers, then train your model. If you model underperforms, try un-freezing the topmost frozen layer\n",
    "* The more data you have the more you can unfreeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model Zoos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Zoos contain models trained by other people. Look in [TensorFlows model zoo](https://github.com/tensorflow/models) or in [Caffe's model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo) (look [here](https://github.com/ethereon/caffe-tensorflow) for help converting from Caffe models to tensorflow models) for models to base your projects on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unsupervised Pretraining*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Pretraining on an Auxillary Task*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
