{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapter Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Vanishing Gradients Problem** is a side effect of applying the Gradient Descent algorithm to deep neural networks wherein the back propogation error of the lower layers is so low that the algorithm leaves the weights of those layers virtually unchanged throughout the training phase. \n",
    "\n",
    "The **Exploding Gradients Problem** is a side effect of applying the Gradient Descent algorithm to deep neural networks wherein the back propogation error of the lower layers is so high that the algorithm updates the weights of those layers so substantially every iteration that the alogirthm diverges.  \n",
    "\n",
    "This problem can occur due to the difference in the variances of the sigmoid activation function and the normal distribution ($\\mu = 0$, $\\sigma = 1$) used to initialize layer weights. The use of these two functions causes the inputs of a layer to have lower variance than the output of the same layer. This difference eventually propogates enough that output variance is considerably higher in deeper layers when compared to the variance of the begining layers. This causes the inputs to have exteremly positive or negative values which in turn results in a derivative approcahing 0. Such a low derivative means the weights of a layer will remain unchanged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Xavier and He Initialization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a signal to flow properly through a DNN, the variance of the inputs and outputs of each layer must remain virtually the same going in both directions (forward and back propogation). However this is in practice when the layer does not have an equal amount of input and output connections. Instead the connection weights should be intialized following the *Xavier Initlization* strategy when using a logistic activation function:\n",
    "\n",
    "\n",
    "$\\large \\text{Normal Distribution w/ } \\mu = 0 \\text{ and } \\sigma = \\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ \n",
    "\n",
    "OR\n",
    "\n",
    "$\\large \\text{Uniform Distribution between } [ -r, +r ] \\text{ where } r = \\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$ \n",
    "\n",
    "Using this strategy speeds up the training of DNNs. For the ReLU activation function you can use *He activation*. By default the $fully\\_connected()$ function uses Xavier initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Nonsaturating Activation Function*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploding/vanishing gradients problems are in part due to a poor choice in activation function. The ReLU function behaves better in DNNs (as it does not saturate for positive values) when compared to the logistic function but it does have some drawbacks. An example of this is the **dying ReLU** problem wherein some nodes in your DNN simply die and output 0 no matter the input. \n",
    "\n",
    "The ReLU function has some variants. The *Leaky ReLU* function ($\\text{LeakyReLU}_{\\alpha}(z) = \\text{max}(\\alpha z, \\, z)$) uses the hyperparameter $\\alpha$ to avoid the problem of dying nodes. However it can result in nodes being in a coma, which essentially means they can die but come back to life if given enough time. \n",
    "\n",
    "Another activation function is the *exponential linear unit* (ELU): \n",
    "\n",
    "$\\large \\begin{gather*} \\text{ELU}_\\alpha (z) =\n",
    "\\begin{cases}\n",
    "  \\alpha \\, (e^{\\,z} - 1) & \\text{ if } z < 0\\\\    \n",
    "  z & \\text{ if } z \\geq 0 \\\\     \n",
    "\\end{cases}\n",
    "\\end{gather*}$\n",
    "\n",
    "The ELU function in general has an average output close to 0 (so no vanishing gradients problem), has a nonzero gradient when $z<0$ so nodes won't die, and is smooth everywhere so it speeds up convergence. However it is slower to compute than the ReLU function but this can be mitigated as it converges faster during training.  Both the leaky ReLU and the ELU functions can be used in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "def leaky_rlu(z, name=None):\n",
    "    return tf.maximum(0.01*z, z, name=name) \n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu) # ELU\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_rlu) # ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batch Normalization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Batch Normalization** algorithm consists of adding an operation in the model just before the activation of each layer, simply zero-centering and normalizing the inputs, then scaling and shifting the result using two new parameters per layer. This reduces the chances of the exploding/vanishing gradients problem occuring late into training. The algorithm is given below:\n",
    "\n",
    "1. $\\large \\quad \\mu_B = \\frac{1}{m_b} \\, \\Sigma_{i=1}^{m_B} \\, x^{(i)}$\n",
    "2. $\\large \\quad \\sigma_B^2 = \\frac{1}{m_b} \\, \\Sigma_{i=1}^{m_B} \\, (x^{(i)} - \\mu_B)^2$\n",
    "3. $\\large \\quad \\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
    "4. $\\large \\quad z^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta$\n",
    "\n",
    "Using Batch Normalization will make your DNN more acurate, make it converge in fewer training iterations, and eliminate the need for regularization. However it make predictions slower, as there are more computations, and it makes your model more complex. \n",
    "\n",
    "Here's an implementation of this algorithm in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last few lines are very repatative, this can be fixed using Python's partial function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the $training$ variable, when your model is making predictions you should remember to set this parameter to false. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Gradient Clipping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Gradient Clipping** technique is a technique wherein the backpropgation gradient is limited to some max and is *clipped* if the gradient is higher than the max. \n",
    "\n",
    "Do do this in tensorflow, you must clip the gradient value before calling $minimize()$ as that function will compute and apply an gradient. Therefore you must ask tensorflow to compute the gradient, clip it, and then apply it to the layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss) # Compute\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) # Clip\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs) # Apply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should always try to borrow layers from another DNN that tackles a problem similar to yours. This will make training faster as you can fix the weights of those layers and only train the new portion of the DNN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reusing a TensorFlow Model*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is an example of reusing only specific portions of a previously trained TensorFlow model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Reusing Models from Other Frameworks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model uses another framework then you will have to load the weights into Python and assign to a layer in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [...] # Load the weights from the other framework\n",
    "original_b = [...] # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Freezing the Lower Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ignore a layer, simply tell the $minimize()$ function which variables to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Tweaking, Dropping, or Replacing the Upper Layers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You almost always have to drop the output layer\n",
    "* Freeze all copied layers, then train your model. If you model underperforms, try un-freezing the topmost frozen layer\n",
    "* The more data you have the more you can unfreeze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model Zoos*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Zoos contain models trained by other people. Look in [TensorFlows model zoo](https://github.com/tensorflow/models) or in [Caffe's model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo) (look [here](https://github.com/ethereon/caffe-tensorflow) for help converting from Caffe models to tensorflow models) for models to base your projects on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Unsupervised Pretraining*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can preform *unsupervised pre-training* if you cannot find a similar model in any model zoo and you don't have much labeled data. Research *Restricted Boltzmann Machines* and *autoencoders* for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Pretraining on an Auxillary Task*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Another option is train a network for a related task and re-use the bottom layers. \n",
    "\n",
    "If need more labeled training data, gather all your current labeled data and alter in some way to change the classification. This is often faster and cheaper than manually altering data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four ways to speed up training that we have covered so far:\n",
    "1. Using a good initailization strategies for the weights\n",
    "2. Using a good activation function\n",
    "3. Using Batch Normalization\n",
    "4. Reusing parts of an old network\n",
    "\n",
    "All of these methods still use Gradient Descent to change the weights associated with a neuron. In this section we will cover optimizers that are generally faster than Gradient Descent. Having said that, we shall see that the **Adam optimizer** generally works best for training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Momentum Optimization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a **momentum optimizer**, the current gradient is used in conjunction with a *momentum vector*, $m$, to determine the change in a neuron weights. In physics terms, the current gradient is the acceleration and the momentum vector would be the current velocity. This system introduces a new hyperparameter $\\beta$ that acts analogous to friction. \n",
    "\n",
    "$\\large m \\leftarrow \\beta m + \\eta \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "$\\large \\theta \\leftarrow \\theta - m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Nesterov Accelerated Gradient*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Nesterov Accelerated Gradient** is an improved version of the momentum optimizer that measures the gradient not at the local position but at the position slightly ahead in the direction of the momentum vector. In general, it is much faster than regular momentum optimization.\n",
    "\n",
    "$\\large m \\leftarrow \\beta m + \\eta \\nabla_\\theta J(\\theta + \\beta m)$\n",
    "\n",
    "$\\large \\theta \\leftarrow \\theta - m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Ada Grad*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Ada Grad optimizer** reduces the learning rate when it encounters steep slopes. It does this in order avoid slowing down in valleys and to converge the global minimum faster. \n",
    "\n",
    "$\\large s \\leftarrow s + \\nabla_\\theta J(\\theta) \\, \\otimes \\, \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "$\\large \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\, \\oslash \\, \\sqrt{s + \\epsilon}$\n",
    "\n",
    "The first equation accumulates the square of the gradients into the vector $s$. Thus $s_i$ gets larger and larger if $\\theta_i$ is steep. The second equation is like gradient descent with a scale factor. \n",
    "\n",
    "In general AdaGrad stops too early when training networks so it should not be used to train DNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *RMSProp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prefered optimizer before Adma optimizers were created, **RMSProp** fixes the early stopping issues of AdaGrad by only considering the last few gradients rather than all of them. \n",
    "\n",
    "$\\large s \\leftarrow \\beta s + (1 - \\beta) \\nabla_\\theta J(\\theta) \\, \\otimes \\, \\nabla_\\theta J(\\theta)$\n",
    "\n",
    "$\\large \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta J(\\theta) \\, \\oslash \\, \\sqrt{s + \\epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Adam Optimization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Adaptive Momentum Estimation Optimizer** combines aspects of both Momentum optimization (remebers past gradients) and RMSProp (tracks decaying squared-average of past gradients): \n",
    "\n",
    "1. $\\large m \\leftarrow \\beta_1 m + (1 - \\beta_1) \\nabla_\\theta J(\\theta)$\n",
    "2. $\\large s \\leftarrow \\beta_2 s + (1 - \\beta_2) \\nabla_\\theta J(\\theta) \\, \\otimes \\, \\nabla_\\theta J(\\theta)$\n",
    "3. $\\large m \\leftarrow \\frac{m}{1 - \\beta_1^T}$\n",
    "4. $\\large s \\leftarrow \\frac{s}{1 - \\beta_2^T}$\n",
    "5. $\\large \\theta \\leftarrow \\theta - \\eta m \\, \\oslash \\, \\sqrt{s + \\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $T$ is the iteration number starting at 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Learning Rate Scheduling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Schedules** are ways to change the learning rate of an algorithm during training. Here are a few examples:\n",
    "\n",
    "1. Performance Scheduling: measure the validation error every $N$ steps and reduce the learning rate and reduce the learning rate by $\\lambda$ once the error stops dropping\n",
    "2. Exponential Scheduling: set the learning rate to a function of iteration number $t$: $\\eta(t) = \\eta_0 10^{-t/r}$. Requires finding the optimal values for the $r$ and $\\eta_0$ hypperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Early Stopping*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Early Stopping*: stop training after performance on validation set set begins to drop. Works good enough in practice by itself but should be used in conjunction with another techinque for best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * $l_1$ and $l_2$ Regularization *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [here](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/) for refresher on this subject. $l_1$ regularization can be done as follows in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2) + ... + tf.reduce_sum(tf.abs(WN)))\n",
    "loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, using TensorFlows bultin features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to add your regularized losses!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Dropout*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Dropout*: at every training set every neuron (including input, excluding output) has a probability $p$ of being ignored that step. The $p$ hyperparameter is typically set to 50% and is called the *dropout rate*. This technique can result in an increase of 1% or 2% in accuracy even for state-of-the-art DNNs. \n",
    "\n",
    "This technique works becuase neurons now must be as useful as possible on their own as their neighbors could disappear. They cannot rely on just a few input weights, since those may disappear during some iterations, they must instead pay attention to all their inputs. \n",
    "\n",
    "Note: after training you must multiply each weight by the *keep probability* $(1-p)$ in order to compensate for the sudden increase of input neurons connected to each neuron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Max-Norm Regularization*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each nueron, constrain the weights $w$ of the incoming connections such that $\\Vert w \\Vert_2 \\leq r$ where $r$ is the max-norm hyperparameter and $\\Vert \\cdot \\Vert_2$ is the $l_2$ norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data Augmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
